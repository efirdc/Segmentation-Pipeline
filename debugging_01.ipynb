{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "velvet-spray",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from utils import load_module\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_module(\"./configs/diffusion_hippocampus.py\")\n",
    "\n",
    "variables = dict(DATASET_PATH=\"X:/Datasets/Diffusion_MRI/\", CHECKPOINTS_PATH=\"X:/Checkpoints/\")\n",
    "context = config.get_context(device, variables)\n",
    "context.init_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_module(\"./configs/qsm_deep_grey_matter.py\")\n",
    "\n",
    "variables = dict(DATASET_PATH=\"X:/Datasets/DGM/segmentation_3T_ps18_v3/\", CHECKPOINTS_PATH=\"X:/Checkpoints/\")\n",
    "context = config.get_context(device, variables)\n",
    "context.init_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "secondary-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_module(\"./configs/msseg2.py\")\n",
    "\n",
    "variables = dict(DATASET_PATH=\"X:/Datasets/MSSEG2_processed/\", CHECKPOINTS_PATH=\"X:/Checkpoints/\")\n",
    "context = config.get_context(device, variables)\n",
    "context.init_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "crude-nepal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Context msseg2 created at 210705-185503\n",
       "\n",
       "component_id=0\n",
       "component_defintition={   'constructor': <class 'data_processing.subject_folder.SubjectFolder'>,\n",
       "    'name': 'dataset',\n",
       "    'params': {   'cohorts': {   'all': <data_processing.subject_filters.RequireAttributes object at 0x00000274D548ED00>,\n",
       "                                 'training': <data_processing.subject_filters.ComposeFilters object at 0x00000274D548EE20>,\n",
       "                                 'validation': <data_processing.subject_filters.RequireAttributes object at 0x00000274D548EE80>},\n",
       "                  'root': '$DATASET_PATH',\n",
       "                  'subject_loader': <data_processing.subject_loaders.ComposeLoaders object at 0x00000274D548ECA0>,\n",
       "                  'subject_path': '',\n",
       "                  'transforms': {   'default': Compose([Compose([SetDataType(data_type=torch.float32, intensity_only=True), CropToMask(label_map_name=brain_mask, label_id=1, label_channel=0), Resample(target=1, image_interpolation=linear, pre_affine_name=None, scalars_only=False), CropOrPad(), RescaleIntensity(out_min_max=(-1, 1.0), percentiles=(0.0, 99.5), masking_method=None), ConcatenateImages(image_names=['flair_time01', 'flair_time02'], image_channels=[1, 1], new_image_name=X), RenameProperty(old_name=ground_truth, new_name=y), CustomOneHot(num_classes=-1)])]),\n",
       "                                    'training': Compose([Compose([SetDataType(data_type=torch.float32, intensity_only=True), CropToMask(label_map_name=brain_mask, label_id=1, label_channel=0), Resample(target=1, image_interpolation=linear, pre_affine_name=None, scalars_only=False), CropOrPad(), RescaleIntensity(out_min_max=(-1, 1.0), percentiles=(0.0, 99.5), masking_method=None), ConcatenateImages(image_names=['flair_time01', 'flair_time02'], image_channels=[1, 1], new_image_name=X), RenameProperty(old_name=ground_truth, new_name=y), CustomOneHot(num_classes=-1)])])}}}\n",
       "component=<data_processing.subject_folder.SubjectFolder object at 0x00000274D549C910>\n",
       "\n",
       "component_id=1\n",
       "component_defintition={   'constructor': <class 'models.NestedResUNet'>,\n",
       "    'name': 'model',\n",
       "    'params': {   'dropout_p': 0.2,\n",
       "                  'filters': 40,\n",
       "                  'input_channels': 2,\n",
       "                  'output_channels': 2,\n",
       "                  'saggital_split': False}}\n",
       "component=NestedResUNet(\n",
       "  (dropout): Dropout3d(p=0.2, inplace=False)\n",
       "  (down): AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
       "  (up): Upsample(scale_factor=2.0, mode=trilinear)\n",
       "  (conv0_0): Block(\n",
       "    (res_conv): Conv3d(2, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (conv1): Conv3d(2, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation1): ReLU(inplace=True)\n",
       "    (conv2): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn2): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation2): ReLU(inplace=True)\n",
       "    (dropout): Dropout3d(p=0.2, inplace=False)\n",
       "  )\n",
       "  (conv1_0): Block(\n",
       "    (conv1): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation1): ReLU(inplace=True)\n",
       "    (conv2): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn2): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation2): ReLU(inplace=True)\n",
       "    (dropout): Dropout3d(p=0.2, inplace=False)\n",
       "  )\n",
       "  (conv0_1): Block(\n",
       "    (res_conv): Conv3d(80, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (conv1): Conv3d(80, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation1): ReLU(inplace=True)\n",
       "    (conv2): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn2): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation2): ReLU(inplace=True)\n",
       "    (dropout): Dropout3d(p=0.2, inplace=False)\n",
       "  )\n",
       "  (conv2_0): Block(\n",
       "    (conv1): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation1): ReLU(inplace=True)\n",
       "    (conv2): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn2): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation2): ReLU(inplace=True)\n",
       "    (dropout): Dropout3d(p=0.2, inplace=False)\n",
       "  )\n",
       "  (conv1_1): Block(\n",
       "    (conv1): Conv3d(120, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation1): ReLU(inplace=True)\n",
       "    (conv2): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn2): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation2): ReLU(inplace=True)\n",
       "    (dropout): Dropout3d(p=0.2, inplace=False)\n",
       "  )\n",
       "  (conv0_2): Block(\n",
       "    (res_conv): Conv3d(80, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (conv1): Conv3d(80, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation1): ReLU(inplace=True)\n",
       "    (conv2): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn2): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation2): ReLU(inplace=True)\n",
       "    (dropout): Dropout3d(p=0.2, inplace=False)\n",
       "  )\n",
       "  (conv3_0): Block(\n",
       "    (conv1): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation1): ReLU(inplace=True)\n",
       "    (conv2): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn2): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation2): ReLU(inplace=True)\n",
       "    (dropout): Dropout3d(p=0.2, inplace=False)\n",
       "  )\n",
       "  (conv2_1): Block(\n",
       "    (conv1): Conv3d(120, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation1): ReLU(inplace=True)\n",
       "    (conv2): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn2): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation2): ReLU(inplace=True)\n",
       "    (dropout): Dropout3d(p=0.2, inplace=False)\n",
       "  )\n",
       "  (conv1_2): Block(\n",
       "    (conv1): Conv3d(120, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation1): ReLU(inplace=True)\n",
       "    (conv2): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn2): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation2): ReLU(inplace=True)\n",
       "    (dropout): Dropout3d(p=0.2, inplace=False)\n",
       "  )\n",
       "  (conv0_3): Block(\n",
       "    (res_conv): Conv3d(80, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (conv1): Conv3d(80, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation1): ReLU(inplace=True)\n",
       "    (conv2): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn2): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation2): ReLU(inplace=True)\n",
       "    (dropout): Dropout3d(p=0.2, inplace=False)\n",
       "  )\n",
       "  (out_conv): Conv3d(40, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (hypothesis): Softmax(dim=1)\n",
       ")\n",
       "\n",
       "component_id=2\n",
       "component_defintition={   'constructor': <class 'torch.optim.adam.Adam'>,\n",
       "    'name': 'optimizer',\n",
       "    'params': {'lr': 0.001, 'params': 'self.model.parameters()'}}\n",
       "component=Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0\n",
       ")\n",
       "\n",
       "component_id=3\n",
       "component_defintition={   'constructor': <class 'evaluation.HybridLogisticDiceLoss'>,\n",
       "    'name': 'criterion',\n",
       "    'params': {'logistic_weights': [1, 100]}}\n",
       "component=HybridLogisticDiceLoss()\n",
       "\n",
       "component_id=4\n",
       "component_defintition={   'constructor': <class 'segmentation_training.SegmentationTrainer'>,\n",
       "    'name': 'trainer',\n",
       "    'params': {   'max_iterations_with_no_improvement': 500,\n",
       "                  'one_time_evaluators': [],\n",
       "                  'save_folder': '$CHECKPOINTS_PATH',\n",
       "                  'save_rate': 100,\n",
       "                  'scoring_function': <function get_context.<locals>.scoring_function at 0x00000274D5497310>,\n",
       "                  'scoring_interval': 50,\n",
       "                  'training_batch_size': 1,\n",
       "                  'training_evaluators': [   <segmentation_training.ScheduledEvaluation object at 0x00000274D549C250>,\n",
       "                                             <segmentation_training.ScheduledEvaluation object at 0x00000274D549C310>],\n",
       "                  'validation_evaluators': [   <segmentation_training.ScheduledEvaluation object at 0x00000274D549C3D0>,\n",
       "                                               <segmentation_training.ScheduledEvaluation object at 0x00000274D549C490>,\n",
       "                                               <segmentation_training.ScheduledEvaluation object at 0x00000274D549C520>,\n",
       "                                               <segmentation_training.ScheduledEvaluation object at 0x00000274D549C5B0>,\n",
       "                                               <segmentation_training.ScheduledEvaluation object at 0x00000274D549C640>,\n",
       "                                               <segmentation_training.ScheduledEvaluation object at 0x00000274D549C6D0>,\n",
       "                                               <segmentation_training.ScheduledEvaluation object at 0x00000274D549C760>,\n",
       "                                               <segmentation_training.ScheduledEvaluation object at 0x00000274D549C7F0>,\n",
       "                                               <segmentation_training.ScheduledEvaluation object at 0x00000274D549C880>]}}\n",
       "component=<segmentation_training.SegmentationTrainer object at 0x00000274D550FA60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "addressed-browse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G:\\\\Github Repositories\\\\Segmentation-Pipeline'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "import os\n",
    "#print(inspect.getsourcefile(context.model.__class__))\n",
    "\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-restriction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-agreement",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasd_dataset = context.dataset.get_cohort_dataset('fasd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-poison",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasd_dataset.preload_and_transform_subjects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasd_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "untransformed_subject = context.dataset.subjects[0]\n",
    "print(\"Original labels:\")\n",
    "print(untransformed_subject['whole_roi']['label_values'])\n",
    "\n",
    "subject = context.dataset[0]\n",
    "print(\"\\nTransformed labels:\")\n",
    "print(subject['y']['label_values'])\n",
    "\n",
    "inverse_subject = subject.apply_inverse_transform(warn=False)\n",
    "print(\"\\nInverse transformed labels:\")\n",
    "print(inverse_subject['y']['label_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subject[\"X\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-borough",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in context.dataset[1].items():\n",
    "    print(key, value, type(value))\n",
    "    if isinstance(value, dict):\n",
    "        for key, value2 in value.items():\n",
    "            print(key, value2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(loader):\n",
    "    while True:\n",
    "        for batch in loader:\n",
    "            yield batch\n",
    "           \n",
    "for i in range(2):\n",
    "    loader = sample_data(context.dataloader)\n",
    "    batch = next(loader)\n",
    "    for key, val in batch.items():\n",
    "        print(key, val.shape, val.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(20, 20, 20)\n",
    "for y in x.split([3, 2, 5, 10]):\n",
    "    print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "determined-anchor",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e087188365de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwandb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.run.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(\n",
    "    project=\"test-project-2\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    log_dict = {\n",
    "        'something': {\n",
    "            foo: {'mean': random.random(), 'std': random.random()}\n",
    "            for foo in \"ABC\"\n",
    "        }\n",
    "    }\n",
    "    wandb.log(log_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-communication",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "for i in range(10):\n",
    "    evaluation = {}\n",
    "    \n",
    "    for structure in (\"A\", \"B\", \"C\"):\n",
    "        columns = [\"TP\", \"FP\", \"TN\", \"FN\", 'dice', \"jaccard\"]\n",
    "        S = 25\n",
    "        subjects = [f'subject_{i:01}' for i in range(S)]\n",
    "\n",
    "        df = pd.DataFrame(data=np.random.randint(50, 100, size=(S, 4)), columns=[\"TP\", \"FP\", \"TN\", \"FN\"])\n",
    "        df.insert(loc=0, column='Subject', value=subjects)\n",
    "\n",
    "        TP, FP, TN, FN = df[\"TP\"], df[\"FP\"], df[\"TN\"], df[\"FN\"]\n",
    "\n",
    "        df['dice'] = 2 * TP / (2 * TP + FP + FN)\n",
    "        df['jaccard'] = TP / (TP + FP + FN)\n",
    "        evaluation[f\"Structure {structure}\"] = wandb.Table(dataframe=df)\n",
    "        \n",
    "    wandb.log({f'Segmentation Evaluation': evaluation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-protocol",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'X:\\Datasets\\Diffusion_MRI\\Attributes\\demographics.xlsx'\n",
    "df = pd.read_excel(file_path, index_col=0)\n",
    "#subject_col = df.columns[0]\n",
    "data = df.to_dict(orient='dict')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "print(pathlib.Path('yourPath.example').suffix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = 'X:\\Datasets\\DGM\\subject_attributes\\dgm_label_names.json'\n",
    "with open(file_path) as f:\n",
    "    data = json.load(f)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = 'X:\\Datasets\\DGM\\subject_attributes\\dgm_label_names.csv'\n",
    "df = pd.read_csv(file_path, index_col=0)\n",
    "data = df.to_dict()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-editor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import is_sequence\n",
    "\n",
    "def test_func(*args):\n",
    "    if is_sequence(args) and len(args) == 1 and is_sequence(args[0]):\n",
    "        args = args[0]\n",
    "    print(args)\n",
    "    \n",
    "test_func(\"a\", \"b\", \"c\")\n",
    "test_func((\"a\", \"b\", \"c\"))\n",
    "test_func([\"a\", \"b\", \"c\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([1, 0, 1]).bool()\n",
    "b = torch.tensor([1, 1, 0]).bool()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "import torch\n",
    "from evaluators import SegmentationEvaluator\n",
    "import pandas as pd\n",
    "\n",
    "def pd_to_dict(elem):\n",
    "    if isinstance(elem, dict):\n",
    "        return {\n",
    "            key: pd_to_wandb(val)\n",
    "            for key, val in elem.items()\n",
    "        }\n",
    "    elif isinstance(elem, pd.DataFrame):\n",
    "        return elem.to_dict()\n",
    "    return elem\n",
    "\n",
    "def pd_to_wandb(elem):\n",
    "    if isinstance(elem, dict):\n",
    "        return {\n",
    "            key: pd_to_wandb(val)\n",
    "            for key, val in elem.items()\n",
    "        }\n",
    "    elif isinstance(elem, pd.DataFrame):\n",
    "        return wandb.Table(dataframe=elem)\n",
    "    return elem\n",
    "\n",
    "wandb.init(project=\"test-project-4\")\n",
    "for i in range(10):\n",
    "    label_values = {letter: val for val, letter in enumerate(\"ABCDE\")}\n",
    "\n",
    "    subjects = [\n",
    "        tio.Subject({\n",
    "            'name': f'subject_{i:02}',\n",
    "            'pred': tio.LabelMap(\n",
    "                tensor=torch.randint(0, 5, size=(1, 20, 20, 20)),\n",
    "                label_values=label_values\n",
    "            ),\n",
    "            'target': tio.LabelMap(\n",
    "                tensor=torch.randint(0, 5, size=(1, 20, 20, 20)),\n",
    "                label_values=label_values\n",
    "            ),\n",
    "        })\n",
    "        for i in range(20)\n",
    "    ]\n",
    "\n",
    "    seg_evaluator = SegmentationEvaluator(\n",
    "        prediction_label_name='pred', target_label_name='target', stats_to_output=['FP', 'TP', 'dice'], \n",
    "    )\n",
    "\n",
    "    log_dict = seg_evaluator(subjects)\n",
    "    wandb.log(pd_to_wandb(log_dict))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = tio.LabelMap(\n",
    "    tensor=torch.randint(0, 5, size=(1, 20, 20, 20)),\n",
    "    label_values=label_values\n",
    ")\n",
    "\n",
    "tio.OneHot()(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10).unsqueeze(1)\n",
    "mask = torch.tensor([0, 1, 1, 0]).bool().unsqueeze(0)\n",
    "x, mask = torch.broadcast_tensors(x, mask)\n",
    "\n",
    "print(x.shape, mask.shape)\n",
    "\n",
    "x[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluators import VolumeStatsEvaluator\n",
    "import pandas as pd\n",
    "import torchio as tio\n",
    "\n",
    "\n",
    "label_values = {f'label_{letter}': (val + 1) for val, letter in enumerate(\"abcde\")}\n",
    "\n",
    "subjects = [\n",
    "    tio.Subject({\n",
    "        'name': f'subject_{i:02}',\n",
    "        'label': tio.LabelMap(\n",
    "            tensor=torch.randint(0, 5, size=(1, 20, 20, 20)),\n",
    "            label_values=label_values\n",
    "        ),\n",
    "        'md': tio.ScalarImage(\n",
    "            tensor=torch.randn(size=(1, 20, 20, 20)) * 2 + 1,\n",
    "        ),\n",
    "        'fa': tio.ScalarImage(\n",
    "            tensor=torch.randn(size=(1, 20, 20, 20)) * 0.5 - 3,\n",
    "        ),\n",
    "    })\n",
    "    for i in range(20)\n",
    "]\n",
    "\n",
    "volume_stats_eval = VolumeStatsEvaluator(\n",
    "    label_map_name='label', image_names=['md', 'fa']\n",
    ")\n",
    "\n",
    "volume_stats_eval(subjects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "def merge_dicts(a, b):\n",
    "    merged_dict = {}\n",
    "    for key in a.keys():\n",
    "        if key in b:\n",
    "            merged_dict[key] = merge_dicts(a[key], b[key])\n",
    "        else:\n",
    "            merged_dict[key] = a[key]\n",
    "    for key in b.keys():\n",
    "        if key not in a:\n",
    "            merged_dict[key] = b[key]\n",
    "    return merged_dict\n",
    "\n",
    "dict1 = {'mean': {'hippo': {'md': {'subject_1': 1}}}}\n",
    "dict2 = {'mean': {'hippo': {'md': {'subject_2': 2}}}}\n",
    "dict3 = {'mean': {'hippo': {'fa': {'subject_1': 3}}}}\n",
    "dict4 = {'mean': {'hippo': {'fa': {'subject_2': 4}}}}\n",
    "\n",
    "out_dict = merge_dicts(dict1, dict2)\n",
    "\n",
    "out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import *\n",
    "\n",
    "train_dataset = context.dataset.get_cohort_dataset(\n",
    "    RequireAttributes({\"pathologies\": \"None\", \"protocol\": \"cbbrain\", \"rescan_id\": \"None\"})\n",
    ")\n",
    "test_dataset = context.dataset.get_cohort_dataset(\n",
    "    'ab300'\n",
    ")\n",
    "\n",
    "print(len(train_dataset), len(test_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for subject in train_dataset.all_subjects:\n",
    "    img_aff = subject[\"mean_dwi\"].affine\n",
    "    label_aff = subject['whole_roi'].affine\n",
    "    diff = img_aff - label_aff\n",
    "    if np.any(np.abs(diff) > 1e-6):\n",
    "        print(subject['name'])\n",
    "        print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import save_dataset_as_nn_unet\n",
    "from data_processing import *\n",
    "\n",
    "save_dataset_as_nn_unet(\n",
    "    context.dataset, \n",
    "    \"X:\\\\Datasets\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task500_DMRI_Hippocampus_Whole\\\\\",\n",
    "    short_name=\"DMRI\", image_names=['mean_dwi', 'md', 'fa'], label_map_name='whole_roi',\n",
    "    train_cohort=RequireAttributes({\"pathologies\": \"None\", \"protocol\": \"cbbrain\", \"rescan_id\": \"None\"}),\n",
    "    test_cohort=None, #\"ab300\",\n",
    "    fix_affine=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context.dataset.all_subjects[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import save_dataset_as_nn_unet\n",
    "\n",
    "save_dataset_as_nn_unet(\n",
    "    context.dataset, \n",
    "    \"X:\\\\Datasets\\\\nnUNet_raw_data_base\\\\nnUNet_raw_data\\\\Task502_MSSEG2\\\\\",\n",
    "    short_name=\"MSSEG2\", image_names=['flair_time01', 'flair_time02'], label_map_name='ground_truth',\n",
    "    train_cohort = 'all'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = None\n",
    "{1:2, **({} if thing is None else thing), 3:4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-adoption",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(RequireAttributes({\"pathologies\": \"None\", \"protocol\": \"cbbrain\", \"rescan_id\": \"None\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torchio as tio\n",
    "\n",
    "subject = context.dataset.all_subjects_map['cbbrain_192']\n",
    "subject = copy.deepcopy(subject)\n",
    "\n",
    "img_aff = subject[\"mean_dwi\"].affine\n",
    "label_aff = subject['whole_roi'].affine\n",
    "print(img_aff - label_aff)\n",
    "\n",
    "subject['whole_roi'].affine = subject['mean_dwi'].affine\n",
    "\n",
    "img_aff = subject[\"mean_dwi\"].affine\n",
    "label_aff = subject['whole_roi'].affine\n",
    "print(img_aff - label_aff)\n",
    "\n",
    "subject['whole_roi'].save('test_label.nii.gz')\n",
    "\n",
    "loaded_label = tio.LabelMap('test_label.nii.gz')\n",
    "print(img_aff - loaded_label.affine)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
